<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 深度学习从入门到入土 | Wenxuan Dong </title> <meta name="author" content="Wenxuan Dong"> <meta name="description" content="深度学习笔记"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/web_icon.png?eb2dd1a5d170b5c7e68079cdcf77a8de"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wxndong.github.io/posts/DeepLearning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wenxuan</span> Dong </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">深度学习从入门到入土</h1> <p class="post-meta"> Created in January 27, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> Research   <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning   ·   <i class="fa-solid fa-tag fa-sm"></i> AI </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="1-线性神经网络">1 线性神经网络</h1> <h2 id="11-梯度">1.1 梯度</h2> <p>梯度是数学和机器学习中的一个重要概念，尤其是在优化问题和深度学习中。梯度是一个向量，表示一个函数在某一点处的方向导数，指向函数值增长最快的方向。在多变量函数中，梯度是各个偏导数组成的向量。</p> <h3 id="111-梯度的定义">1.1.1 梯度的定义</h3> <p>对于一个多变量函数 $f(x_1, x_2, \dots, x_n)$，其梯度是一个向量，记作 $\nabla f$，其中每个分量是函数对相应变量的偏导数：</p> \[\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)\] <h3 id="112-为什么要计算梯度">1.1.2 为什么要计算梯度</h3> <ol> <li> <strong>优化问题</strong>：在机器学习和深度学习中，我们通常需要最小化一个损失函数。梯度提供了函数值下降最快的方向，因此我们可以通过沿着梯度的反方向调整参数来最小化损失函数。这种方法称为梯度下降法。</li> <li> <strong>模型训练</strong>：在训练神经网络时，我们通过反向传播算法计算损失函数对每个参数的梯度。然后使用这些梯度来更新模型的参数，使得模型在训练数据上的表现逐渐改善。</li> <li> <strong>理解和分析</strong>：梯度可以帮助我们理解函数的行为。例如，梯度为零的点可能是函数的极小值、极大值或鞍点。这对于分析模型的收敛性和稳定性非常重要。</li> </ol> <h3 id="113-梯度下降法">1.1.3 梯度下降法</h3> <p>梯度下降法是一种迭代优化算法，用于寻找函数的局部最小值。其基本步骤如下：</p> <ol> <li>初始化参数值。</li> <li>计算损失函数对参数的梯度。</li> <li>沿着梯度的反方向更新参数。</li> <li>重复步骤2和3，直到收敛。</li> </ol> <p>SGD（Stochastic Gradient Descent，随机梯度下降）是一种常用的优化算法，主要用于机器学习和深度学习中的模型训练。其核心思想是通过迭代调整模型参数，最小化损失函数。</p> <ul> <li> <strong>优点</strong> <ul> <li> <strong>计算效率</strong>：每次迭代只使用一个或少量样本，计算速度快，适合大规模数据集。</li> <li> <strong>逃离局部最优</strong>：随机性有助于跳出局部最优，可能找到更好的解。</li> </ul> </li> <li> <strong>缺点</strong> <ul> <li> <strong>收敛不稳定</strong>：由于梯度估计的随机性，损失函数可能波动较大。</li> <li> <strong>超参数敏感</strong>：学习率的选择对性能影响较大。</li> </ul> </li> <li> <strong>变体</strong> <ul> <li> <strong>Mini-batch Gradient Descent</strong>：使用小批量样本计算梯度，平衡了计算效率和稳定性。</li> <li> <strong>Momentum</strong>：引入动量项，加速收敛并减少震荡。</li> <li> <strong>Adam</strong>：结合动量和自适应学习率，表现优异。</li> <li><strong>AdamW</strong></li> </ul> </li> </ul> <h3 id="114-例子">1.1.4 例子</h3> <p>假设我们有一个简单的损失函数 $L(w) = (w - 3)^2$，我们希望找到使 $L(w)$ 最小的 $w$。</p> <ol> <li>计算梯度：$\frac{dL}{dw} = 2(w - 3)$。</li> <li>初始化 $w$ 的值，例如 $w = 0$。</li> <li>更新 $w$：$w = w - \alpha \frac{dL}{dw}$，其中 $\alpha$ 是学习率。</li> <li>重复更新步骤，直到 $w$ 接近3，此时 $L(w)$ 达到最小值。</li> </ol> <p>通过计算和利用梯度，我们可以有效地优化模型参数，提高模型的性能。</p> <h3 id="115-自动求导">1.1.5 自动求导</h3> <ul> <li>大部分时候对标量求导</li> <li>反向传播</li> </ul> <h2 id="12-全连接层线性层">1.2 全连接层（线性层）</h2> <ul> <li> <p>通过矩阵乘法，全连接层将<code class="language-plaintext highlighter-rouge">输入</code>特征<code class="language-plaintext highlighter-rouge">映射</code>到<code class="language-plaintext highlighter-rouge">输出</code>特征。 全连接层的计算公式为： \(y = xW^T + b\)</p> <p>其中：</p> <ul> <li> <strong>输入矩阵 <code class="language-plaintext highlighter-rouge">x</code></strong>：形状为 <code class="language-plaintext highlighter-rouge">(batch_size, input_features)</code>。表示一批样本的特征；</li> <li> <strong>权重矩阵 <code class="language-plaintext highlighter-rouge">W</code></strong>：形状为 <code class="language-plaintext highlighter-rouge">(output_features, input_features)</code>。表示全连接层的权重；</li> <li> <code class="language-plaintext highlighter-rouge">W^T</code> 是 <code class="language-plaintext highlighter-rouge">W</code> 的转置，形状为 <code class="language-plaintext highlighter-rouge">(input_features, output_features)</code>。</li> <li> <code class="language-plaintext highlighter-rouge">b</code> 是偏置向量，形状为 <code class="language-plaintext highlighter-rouge">(output_features,)</code>。</li> <li> <strong>输出矩阵 <code class="language-plaintext highlighter-rouge">y</code></strong>：形状为 <code class="language-plaintext highlighter-rouge">(batch_size, output_features)</code>。表示一批样本的输出。</li> </ul> </li> </ul> <p>PS: 维度表示如 x = batch_size * input_features</p> <h2 id="13-softmax-与-交叉熵">1.3 SoftMax 与 交叉熵</h2> <ul> <li>SoftMax是<strong>激活函数</strong> </li> <li>SoftMax通过同时减去最大值，保持了稳定性（转化为分子分母同时乘最大值的指数，上下不变）；</li> <li>将 Softmax 和交叉熵损失结合在一起计算，避免单独计算 Softmax：![[Pasted image 20250110204055.png]]</li> <li> <p>这种方法避免了直接计算 ( \exp(o_j - \max(o_k)) )，从而提高了数值稳定性。</p> </li> <li> <p>关于<code class="language-plaintext highlighter-rouge">极大似然估计</code>的直观理解：如果把所有的所有输出的取值集合看作一个事件，那么将每个输出取到对应值的概率相乘就是这个事件发生的概率，因为“任何已经发生的事件都有其最大概率”，所以需要极大化这个事件概率，这个事件发生的概率就是似然函数，然后如何极大化似然函数，求导，令导数等于0，然后解出未知参数，这样解出的未知参数就叫做参数的似然估计值。然后根据softmax的定义，它就是每个输出取到某个值的一种映射概率，所以似然函数就是将所有输出的softmax相乘，然后<strong>极大化似然函数等价于极小化似然函数的负对数</strong>，那么将似然函数取负对数，根据对数函数的性质，连乘变成连加，这样得到的函数就是<code class="language-plaintext highlighter-rouge">交叉熵</code>。</p> </li> <li> <strong>Softmax</strong> 和 <strong>交叉熵损失</strong> 是深度学习中常用的两个概念，通常一起用于分类任务。它们之间的关系非常紧密，尤其是在多分类问题中。</li> </ul> <hr> <h1 id="2-多层感知机">2 多层感知机</h1> <h2 id="21-感知机">2.1 感知机</h2> <h3 id="211-激活函数">2.1.1 激活函数</h3> <ol> <li>引入非线性，使神经网络能够拟合复杂函数。</li> <li>决定神经元的输出，控制信号传递。</li> <li>增强模型的表达能力。</li> </ol> <h2 id="22-欠拟合过拟合">2.2 欠拟合、过拟合</h2> <p>模型和数据规模不匹配导致的</p> <h2 id="23-权重衰退">2.3 权重衰退</h2> <p><strong>权重衰减</strong>和<strong>L2正则化</strong>是防止模型过拟合的常用方法，本质上是相同的技术。</p> <h3 id="231-l2正则化">2.3.1 <strong>L2正则化</strong> </h3> <p>L2正则化通过在损失函数中加入模型权重的平方和（L2范数），限制权重的大小，防止模型过度依赖某些特征。公式如下：</p> \[\text{损失函数} = \text{原始损失函数} + \frac{\lambda}{2} \sum_{i} w_i^2\] <ul> <li>$w_i$ 是模型的权重。</li> <li>$\lambda$ 是正则化强度，控制正则化项的影响。</li> </ul> <p>L2正则化倾向于让权重较小且分布均匀，避免某些权重过大。</p> <h3 id="232-权重衰减">2.3.2 <strong>权重衰减</strong> </h3> <p>权重衰减是优化过程中直接对权重进行衰减的操作，通常与L2正则化等价。更新权重时的公式为：</p> \[w_i \leftarrow w_i - \eta \left( \frac{\partial \text{损失函数}}{\partial w_i} + \lambda w_i \right)\] <ul> <li>$\eta$ 是学习率。</li> <li>$\lambda$ 是衰减系数，控制衰减强度。</li> </ul> <p>权重衰减在每次更新时都会缩小权重，防止其增长过大。</p> <h3 id="233-总结">2.3.3 总结</h3> <ul> <li> <strong>L2正则化</strong>：在损失函数中加入权重的平方和，限制权重大小。</li> <li> <strong>权重衰减</strong>：在优化过程中直接缩小权重。</li> </ul> <p>两者目标一致，都是通过限制权重大小来防止过拟合。</p> <h2 id="24-丢弃法">2.4 丢弃法</h2> <p>可以视为正则化</p> <h2 id="25-前向传播反向传播和计算图">2.5 前向传播、反向传播和计算图</h2> <p>一方面，在前向传播期间计算正则项 <a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-forward-s" rel="external nofollow noopener" target="_blank">(4.7.5)</a>取决于模型参数$W^{(1)}$和 $W^{(2)}$的当前值。 它们是由优化算法根据最近迭代的反向传播给出的。 另一方面，反向传播期间参数 <a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-backprop-j-h" rel="external nofollow noopener" target="_blank">(4.7.11)</a>的梯度计算， 取决于由前向传播给出的隐藏变量$h$的当前值。</p> <p>因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播<code class="language-plaintext highlighter-rouge">重复利用</code>前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致<code class="language-plaintext highlighter-rouge">内存不足</code>（out of memory）错误。</p> <h3 id="251-小结">2.5.1 小结</h3> <ul> <li>前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li> <li>反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li> <li>在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li> <li>训练比预测需要更多的内存。</li> </ul> <h2 id="26-数值稳定性--模型初始化和激活函数">2.6 数值稳定性 + 模型初始化和激活函数</h2> <h3 id="261-梯度消失和梯度爆炸">2.6.1 梯度消失和梯度爆炸</h3> <p>为了防止梯度消失或梯度爆炸，我们需要确保每层的输出和梯度在合理的范围内。通过合理的权重初始化（如 Xavier 初始化），可以保持每层的输入和输出的方差一致，从而避免梯度在反向传播过程中指数级缩小或放大。Xavier 初始化通过调整权重的分布范围，使得每层的输出和梯度保持在稳定的范围内，进而预防梯度消失和爆炸问题。</p> <h3 id="262-模型初始化和激活函数选择">2.6.2 模型初始化和激活函数选择</h3> <p>激活函数的选择对梯度消失和梯度爆炸问题有重要影响。理想情况下，激活函数在原点附近应满足 $σ(x)≈x$，以保持梯度的稳定性。ReLU 和 Tanh 在原点附近满足这一条件，而 Sigmoid 在远离原点时梯度接近于 0，容易导致梯度消失问题。为了缓解这一问题，可以对 Sigmoid 进行线性变换（如 $4×sigmoid(x)−2$），使其在原点附近的行为更接近线性函数。然而，实际应用中更常见的做法是直接使用 ReLU 或其变体（如 Leaky ReLU、ELU），以有效避免梯度消失问题。</p> <h1 id="3-循环神经网络">3 循环神经网络</h1> <h2 id="31-序列模型">3.1 序列模型</h2> <p>时序序列：不独立 自回归：用自己之前的数据</p> <ul> <li>马尔可夫</li> <li>潜变量</li> </ul> <h2 id="32-文本预处理">3.2 文本预处理</h2> <ul> <li>将文本作为字符串加载到内存中。</li> <li>将字符串拆分为词元（如单词和字符）。</li> <li>建立一个词表，将拆分的词元映射到数字索引。</li> <li>将文本转换为数字索引序列，方便模型操作。</li> </ul> <h3 id="321-tokenize">3.2.1 tokenize</h3> <p>下面的<code class="language-plaintext highlighter-rouge">tokenize</code>函数将文本行列表（<code class="language-plaintext highlighter-rouge">lines</code>）作为输入， 列表中的每个元素是一个文本序列（如一条文本行）。 每个文本序列又被拆分成一个词元列表，<em>词元</em>（token）是文本的基本单位。 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="sh">'</span><span class="s">word</span><span class="sh">'</span><span class="p">):</span>  <span class="c1">#@save
</span>    <span class="sh">"""</span><span class="s">将文本行拆分为单词或字符词元</span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="sh">'</span><span class="s">word</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">token</span> <span class="o">==</span> <span class="sh">'</span><span class="s">char</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">错误：未知词元类型：</span><span class="sh">'</span> <span class="o">+</span> <span class="n">token</span><span class="p">)</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">11</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div> <h3 id="322-vocab">3.2.2 vocab</h3> <ul> <li>vocab是<code class="language-plaintext highlighter-rouge">语料库</code> </li> <li>训练vocab和测试vocab要一致，常见的错误是没有保持训练和测试vocab的一致性</li> </ul> <h3 id="323-embedding">3.2.3 embedding</h3> <p>嵌入层：词转向量 给一个词（词元）token，学习出一个长为d的向量表示它 [[NLP学习笔记]]</p> <h2 id="33-语言模型和数据集">3.3 语言模型和数据集</h2> <h3 id="331-随机采样和顺序分区">3.3.1 随机采样和顺序分区</h3> <p><strong>随机采样：</strong> 在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。</p> <p><strong>顺序分区：</strong> 在迭代过程中，除了对原始序列可以随机抽样外， 我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。</p> <h3 id="332-总结">3.3.2 总结：</h3> <ul> <li>语言模型是自然语言处理的关键。</li> <li>元语法通过截断相关性，为处理长序列提供了一种实用的模型。</li> <li>长序列存在一个问题：它们很少出现或者从不出现。</li> <li>齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他元语法。</li> <li>通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。</li> <li>读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。</li> </ul> <h2 id="34-循环神经网络">3.4 循环神经网络</h2> <p><strong>循环神经网络</strong>（recurrent neural networks，RNNs） 是具有隐状态的神经网络。 用于处理时序序列</p> <h3 id="341-网络模型公式">3.4.1 网络模型公式：</h3> <ul> <li>和MLP之间差一项</li> </ul> <hr> <h3 id="342-困惑度">3.4.2 困惑度：</h3> <p>交叉熵除个均值，带个指数</p> <hr> <h3 id="343-梯度裁剪">3.4.3 梯度裁剪：</h3> <hr> <h3 id="344-总结">3.4.4 总结：</h3> <h2 id="35-循环神经网络代码实现">3.5 循环神经网络代码实现</h2> <h3 id="351-独热编码">3.5.1 独热编码</h3> <p>我们每次采样的小批量数据形状是二维张量： <strong>（批量大小，时间步数）</strong>。 <code class="language-plaintext highlighter-rouge">one_hot</code>函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（<code class="language-plaintext highlighter-rouge">len(vocab)</code>）。 我们经常转换输入的维度，以便获得形状为 <strong>（时间步数，批量大小，词表大小）</strong> 的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">28</span><span class="p">).</span><span class="n">shape</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</code></pre></div></div> <p>1、<strong>转置</strong>输入数据形状的原因：在 RNN 或类似模型中，通常需要按时间步逐个处理数据。默认情况下，输入数据的形状是 <code class="language-plaintext highlighter-rouge">(批量大小, 时间步数)</code>，但这种形状不方便直接按时间步处理数据。 通过转置 <code class="language-plaintext highlighter-rouge">X</code>，可以将数据的形状从 <code class="language-plaintext highlighter-rouge">(批量大小, 时间步数)</code> 变为 <code class="language-plaintext highlighter-rouge">(时间步数, 批量大小)</code>。这样做的目的是：</p> <ul> <li> <strong>方便按时间步处理数据</strong>： <ul> <li>转置后，数据的形状为 <code class="language-plaintext highlighter-rouge">(时间步数, 批量大小)</code>。</li> <li>这样，最外层维度是时间步数，可以直接遍历每个时间步的数据。</li> </ul> </li> <li> <strong>适应 RNN 的输入格式</strong>： <ul> <li>RNN 的输入通常是一个三维张量，形状为 <code class="language-plaintext highlighter-rouge">(时间步数, 批量大小, 特征维度)</code>。</li> <li>转置后，可以更方便地将数据转换为这种格式。</li> </ul> </li> </ul> <p>2、<strong>One-Hot 编码后的形状</strong>： 在转置后，我们对 <code class="language-plaintext highlighter-rouge">X.T</code> 进行 One-Hot 编码：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_one_hot</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">28</span><span class="p">)</span>
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">X.T</code> 的形状是 <code class="language-plaintext highlighter-rouge">(5, 2)</code>。</li> <li>One-Hot 编码后，<code class="language-plaintext highlighter-rouge">X_one_hot</code> 的形状变为 <code class="language-plaintext highlighter-rouge">(5, 2, 28)</code>，其中： <ul> <li> <code class="language-plaintext highlighter-rouge">5</code> 是时间步数。</li> <li> <code class="language-plaintext highlighter-rouge">2</code> 是批量大小。</li> <li> <code class="language-plaintext highlighter-rouge">28</code> 是词表大小。</li> </ul> </li> </ul> <p>这种形状非常适合 RNN 的输入格式，因为：</p> <ul> <li>最外层维度是时间步数，方便按时间步逐个处理数据。</li> <li>每个时间步的数据形状是 <code class="language-plaintext highlighter-rouge">(批量大小, 词表大小)</code>，可以直接输入到 RNN 中。</li> </ul> <h3 id="352-batch_sizenum_stepsepoch">3.5.2 batch_size、num_steps、epoch</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">batch_size</code>：批次大小，训练过程中，数据通常会被划分为多个批次（batch），每个批次包含 <code class="language-plaintext highlighter-rouge">batch_size</code> 个样本。</li> <li> <code class="language-plaintext highlighter-rouge">num_steps</code>：时间步，每次取出的序列长度，也可理解为样本每次输入小批量的序列长度。在 RNN 中，序列数据是按时间步逐步处理的。<code class="language-plaintext highlighter-rouge">steps</code> 决定了 RNN 需要展开的时间步数。</li> <li> <code class="language-plaintext highlighter-rouge">epoch</code>：迭代周期</li> </ul> <h3 id="353-预测">3.5.3 预测</h3> <p>以下函数通过预热期(warm-up)（使用 <code class="language-plaintext highlighter-rouge">prefix</code> 更新隐状态但不生成输出）和预测期（基于更新后的隐状态生成新字符），实现了从给定前缀生成后续字符的功能。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_ch8</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">num_preds</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>  <span class="c1">#@save
</span>    <span class="sh">"""</span><span class="s">在prefix后面生成新字符</span><span class="sh">"""</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="nf">begin_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">prefix</span><span class="p">[</span><span class="mi">0</span><span class="p">]]]</span>
    <span class="n">get_input</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>  <span class="c1"># 预热期
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="nf">get_input</span><span class="p">(),</span> <span class="n">state</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_preds</span><span class="p">):</span>  <span class="c1"># 预测num_preds步
</span>        <span class="n">y</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="nf">get_input</span><span class="p">(),</span> <span class="n">state</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">return</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">vocab</span><span class="p">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span>
</code></pre></div></div> <h3 id="354-梯度裁剪">3.5.4 梯度裁剪</h3> <p>同3.4.3</p> <h3 id="355-模型定义训练">3.5.5 模型定义、训练</h3> <p>训练模型之前，让我们定义一个函数在一个迭代周期内训练模型。</p> <ol> <li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li> <li>我们在更新模型参数之前裁剪梯度。 这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li> <li>我们用困惑度来评价模型。如 <a href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#subsec-perplexity" rel="external nofollow noopener" target="_blank">8.4.4节</a>所述， 这样的度量确保了不同长度的序列具有可比性。</li> </ol> <h1 id="4-现代循环神经网络">4 现代循环神经网络</h1> <h2 id="41-门控单元gru">4.1 门控单元GRU</h2> <h2 id="42-lstm">4.2 LSTM</h2> <p>和GRU效果差不多，稍微好些</p> <h2 id="43-深度循环神经网络">4.3 深度循环神经网络</h2> <p>又往上、又往右</p> <h2 id="44-双向循环神经网络">4.4 双向循环神经网络</h2> <p><em>不适合做推理，适合做句子特征抽取</em></p> <h2 id="45-机器翻译-数据处理">4.5 机器翻译-数据处理</h2> <p>使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。 为了提高计算效率，我们仍然可以通过 <em>截断</em>（truncation）和 <em>填充</em>（padding）方式实现一次只处理</p> <p>一个小批量的文本序列。 假设同一个小批量中的每个序列都应该具有相同的长度<code class="language-plaintext highlighter-rouge">num_steps</code>， 那么如果文本序列的词元数目少于<code class="language-plaintext highlighter-rouge">num_steps</code>时， 我们将继续在其末尾添加特定的“&lt;<strong>pad</strong>&gt;”词元， 直到其长度达到<code class="language-plaintext highlighter-rouge">num_steps</code>； 反之，我们将截断文本序列时，只取其前<code class="language-plaintext highlighter-rouge">num_steps</code> 个词元， 并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度， 以便以相同形状的小批量进行加载。</p> <blockquote> <p>[!BUG] 上面一行中的&lt;pad&gt;如果不加上反斜杠，会导致之后<strong>格式不正常</strong>显示 目前原因未知</p> </blockquote> <h2 id="46-编码器encoder解码器decoder">4.6 编码器Encoder、解码器Decoder</h2> <p>编码器一般双向的，其最后时间步的隐状态作为解码器的初始隐状态</p> <h3 id="461-seq2seq">4.6.1 seq2seq</h3> <p>用于翻译</p> <h3 id="462-束搜索">4.6.2 束搜索</h3> <p>介于贪心和dp之间</p> <h2 id="47-注意力机制">4.7 注意力机制</h2> <p>不随意线索：非主观 随意线索：主观要做的事情</p> <h3 id="471-注意力机制attention-mechanism">4.7.1 <strong>注意力机制（Attention Mechanism）</strong> </h3> <p>注意力机制是一种模拟人类注意力分配的计算方法，用于在输入数据中选择性地关注重要信息。它通过计算 <strong>Query</strong>、<strong>Key</strong> 和 <strong>Value</strong> 之间的关系，决定对哪些部分分配更多注意力。</p> <blockquote> <p>[!Note] 算是与心理学有关</p> </blockquote> <h3 id="472--不随意线索non-volitional-cues">4.7.2 <strong>不随意线索（Non-Volitional Cues）</strong> </h3> <ul> <li> <strong>定义</strong>：不随意线索是指事物本身的固有属性或特征，不需要主观干预即可获取。</li> <li> <strong>对应注意力机制</strong>：在注意力机制中，<strong>Key（K）</strong> 通常对应不随意线索，因为 Key 表示输入数据的固有特征。</li> <li> <strong>示例</strong>：在图像中，物体的颜色、形状等是不随意线索。</li> </ul> <h3 id="473-随意线索volitional-cues">4.7.3 <strong>随意线索（Volitional Cues）</strong> </h3> <ul> <li> <strong>定义</strong>：随意线索是主观的、有意识的选择，表示当前关注的目标或任务。</li> <li> <strong>对应注意力机制</strong>：在注意力机制中，<strong>Query（Q）</strong> 通常对应随意线索，因为 Query 表示当前需要关注的问题或目标。</li> <li> <strong>示例</strong>：在阅读理解任务中，问题是随意线索，因为它引导模型关注文本中的特定部分。</li> </ul> <h3 id="474--queryqkeykvaluev">4.7.4 <strong>Query（Q）、Key（K）、Value（V）</strong> </h3> <ul> <li> <strong>Query（Q）</strong>：表示当前需要关注的目标或问题（随意线索）。</li> <li> <strong>Key（K）</strong>：表示输入数据的固有属性或特征（不随意线索）。</li> <li> <strong>Value（V）</strong>：表示输入数据的实际内容或价值，通常与 Key 相关联。</li> </ul> <p>注意力机制通过计算 <strong>Query</strong> 和 <strong>Key</strong> 的相似度，决定从 <strong>Value</strong> 中提取哪些信息。公式如下：</p> \[Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d_k}})V\] <p>其中：</p> <ul> <li>$QK^T$ 计算 Query 和 Key 的相似度。</li> <li>$\sqrt{d_k}$ 是 Query 和 Key 的维度（即每个向量的长度）。是一个重要的缩放因子，用于调节点积（dot product）的大小。它的作用是防止点积的值过大，导致梯度消失或梯度爆炸问题。</li> <li>$Softmax$ 将相似度转换为权重（概率分布）。</li> <li>$V$ 是加权求和的对象。</li> </ul> <h3 id="475-注意力分数">4.7.5 注意力分数：</h3> <ul> <li> <table> <tbody> <tr> <td>【注意力机制的本质</td> <td>Self-Attention</td> <td>Transformer</td> <td>QKV矩阵】 https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;vd_source=844d5686586c898323ef108d7a5f4a16</td> </tr> </tbody> </table> </li> </ul> <h3 id="476-seq2seq结合注意力机制">4.7.6 seq2seq结合注意力机制</h3> <p>seq2seq加入Attention机制的动机，在于seq2seq本身的缺陷：回忆一下，Encoder 一方面对Decoder输出RNN的hidden state作为Decoder的RNN的初始化hidden state；另一方面，Encoder只把一个句子中的最后一个时刻（word）的最后一层 当做context（上下文），与Decoder的Input之一 Embedding进行拼接，作为Decoder的另一个输入。但是最后一个时刻（word）的最后一层这一context，尽管包含了之前的time_step的信息，但也是间接的信息，而且失去了位置信息。</p> <p>把最后一个时刻（word）的最后一层作为context，是不合适的，因为从道理上讲，Encoder（源语言）和Decoder（目标翻译语言）的位置最好一一对应，而不是统一拿Encoder 的 RNN的最后一个time_step的最后一层当做Decoder的RNN的输入的一部分。举个例子，在Decoder第一个位置预测bonjour的时候，不应该拿Encoder的最后一个位置的句号的输出作为context，而是应该拿第一个位置的hello的输出作为context</p> <p>因此如<strong>4.7.4节 图</strong>所示，引入注意力机制，更全面的获取信息间关系</p> <h2 id="48-transformer">4.8 Transformer</h2> <p>详情见：[[论文精读笔记]] #Transformer</p> <h1 id="5-bert">5 BERT</h1> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Wenxuan Dong. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>